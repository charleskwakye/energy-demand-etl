{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aab5eecf-1221-49b5-bc59-433fca9b74af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config/setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74222ef3-8ab8-4206-b2a5-a8116d247a62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze Layer ETL Process\n",
    "\n",
    "## Why I Made This Notebook\n",
    "I want a simple and smooth way to move raw energy demand data into Delta tables for my ETL pipeline. This step helps me keep the data organized and ready for later transformations. By doing things in this notebook, I avoid messy manual tasks and make everything reproducible.\n",
    "\n",
    "## Getting the Raw Data\n",
    "I use a function to download the raw household power consumption data into the right volume folder. When I'm testing in 'dev', I skip the download if the file already exists. This saves time and keeps things efficient in development.\n",
    "\n",
    "## Reading and Cleaning\n",
    "I load the raw data from a text file, making sure I handle the semicolon separator correctly. I keep all columns as strings in this step to avoid unexpected errors from inferring the schema too early.\n",
    "\n",
    "## Adding Metadata\n",
    "I add columns to track when the data arrived and the file it came from. This makes it easier to debug and trace data issues later on.\n",
    "\n",
    "## Saving to Bronze\n",
    "I write the cleaned and tracked data as a Delta table, overwriting any previous version. This way, I know the bronze layer always has the freshest raw data. I also print the table name so I can see exactly where things were written.\n",
    "\n",
    "By doing all these steps, I know my raw data is safe, organized, and ready for the next part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df612b87-26d6-457b-ab77-e5945f81993b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name, col\n",
    "\n",
    "def download_data_to_volume(url, target_dir):\n",
    "    # In 'dev', skip download if data exists to save time\n",
    "    if env == 'dev' and os.path.exists(target_dir):\n",
    "        print(f\"Skipping download in {env}. Data already exists.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading from {url}...\")\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 200:\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            \n",
    "            # Ensure target directory exists in the Volume\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            \n",
    "            # Extract directly to the Volume path\n",
    "            z.extractall(target_dir)\n",
    "            print(f\"Download and extraction complete at {target_dir}\")\n",
    "        else:\n",
    "            raise Exception(f\"Download failed with status code: {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ingestion: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "download_data_to_volume(raw_data_url, landing_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24933d76-d1fa-426a-b3a3-25ec76aebc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read Raw Data from txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c49089b-e7f6-449e-a52f-61d010287909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "txt_file_path = f\"{landing_path}/household_power_consumption.txt\"\n",
    "\n",
    "# Handle semicolons separators, store as Delta table, Add metadata columns\n",
    "df_raw = (spark.read\n",
    "          .format(\"csv\") # use csv for txt files, it works\n",
    "          .option(\"header\", \"true\")\n",
    "          .option(\"delimiter\", \";\") \n",
    "          .option(\"inferSchema\", \"false\") # Keep as string to avoid schema errors on load\n",
    "          .load(txt_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b5d1bf-1b2e-4690-906c-97d40ee99fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe4045d-adc4-4ff1-8982-705e50aad41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = df_raw \\\n",
    "    .withColumn(\"_ingestion_timestamp\", current_timestamp()) \\\n",
    "    .withColumn(\"_source_file\", col(\"_metadata.file_path\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040ac166-e26b-4de7-a26d-b77d682a6d6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write to bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fadaaa8-fdc8-4e80-b2c7-7bcf4bd8dbe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(full_path_bronze)\n",
    "print(f\"Table saved: {full_path_bronze}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b56c142-a0c3-4953-a909-59f03c50ef49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e282cbaf-b3e5-4c40-a18e-99cefcbce378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
